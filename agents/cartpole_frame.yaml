device: mps

seed: 42
render_upscale_factor: 1
render_layout: [3, 3]

env:
  env_name: CartPole-v1
  env_wrappers:
    - CaptureRenderFrameEnv
    - PreprocessFrameEnv
    - ReturnActionEnv
    - HistoryEnv
  ObservationWrapper:
    input: dense
  CaptureRenderFrameEnv:
    mode: replace
  PreprocessFrameEnv:
    resize_shape: [84, 84]
    grayscale: true
    normalize: true
  HistoryEnv:
    history_length: 4

  env_action_labels: [LEFT, RIGHT]
  num_episodes: 8000
  max_steps_per_episode: 500
  headless: false
  num_envs: 9

agent:
  name: cartpole_frame
  gamma: 0.99
  #apply_noisy_network: true

  epsilon: 0.9
  epsilon_min: 0.05
  epsilon_linear_decay: 0.0000025

  batch_size: 128
  min_memory_size: 128
  loss: huber
  clip_gradients: 10000.0

  # Optimizer settings
  optimizer: adam
  learning_rate: 0.001
  lr_scheduler:
    type: lr_scheduler.LinearLR
    args:
      start_factor: 1
      end_factor: 0.2
      total_iters: 50000

  action_selection: max
  double_dqn: true

  replay_buffer:
    type: prioritized
    size: 5000
    alpha: 0.4
    beta: 0.5
    beta_annealing_steps: 30000

  replay_every_n_steps: 1
  replays_until_target_update: 10
  checkpoint_every_n_replays: 10

  network:
    dueling_dqn:
      convolution:
        type: 2d
        kernel_sizes: [8, 4, 3]
        channels: [32, 64, 64]
        strides: [4, 2, 1]

      value_hidden_layers: [32, 32]
      advantage_hidden_layers: [32, 32]

  disabled_curiosity:
    hidden_layers_dim: [200, 200]
    curiosity_reward_weight: 0.4
    curiosity_reward_exponent: 0.4
