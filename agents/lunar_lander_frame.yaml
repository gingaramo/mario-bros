device: mps

# For reproducibility.
seed: 42

env:
  env_name: "LunarLander-v3"
  env_kwargs:
    continuous: False
    gravity: -10.0
    enable_wind: False
    wind_power: 15.0
    turbulence_power: 1.5
  env_wrappers:
    - CaptureRenderFrameEnv
    - ReturnActionEnv
    - HistoryEnv
  ObservationWrapper:
    input: dense
  CaptureRenderFrameEnv:
    mode: capture  # Options: 'capture', 'replace'
  HistoryEnv:
    history_length: 8

  env_action_labels: [NOOP, LEFT, MAIN, RIGHT]
  num_episodes: 1000
  num_steps: 200000
  headless: false
  num_envs: 4

agent:
  name: lunar_lander_dense
  gamma: 0.99

  apply_noisy_network: true

  batch_size: 512
  min_memory_size: 512
  loss: huber
  clip_gradients: 80.0

  # Optimizer settings
  optimizer: adam
  learning_rate: 0.001
  lr_scheduler:
    type: lr_scheduler.LinearLR
    args:
      start_factor: 1
      end_factor: 0.2
      total_iters: 100000

  action_selection: max
  double_dqn: true

  replay_buffer:
    type: prioritized
    size: 10000
    alpha: 0.4 # Bias towards high surprise
    beta: 0.6 # Importance sampling exponent
    beta_annealing_steps: 20000

  replays_until_target_update: 5
  checkpoint_every_n_replays: 100

  network:
    dueling_dqn:
      value_hidden_layers: [32, 32]
      advantage_hidden_layers: [32, 32]

