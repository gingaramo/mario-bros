device: mps

seed: 42
render_upscale_factor: 2
render_layout: [2, 4]

env:
  env_name: ALE/Pong-v5
  env_wrappers:
    - CaptureRenderFrameEnv
    - PreprocessFrameEnv
    - HistoryEnv
  ObservationWrapper:
    input: dense
  CaptureRenderFrameEnv:
    mode: replace
  PreprocessFrameEnv:
    resize_shape: [84, 84]
    grayscale: true
    normalize: true
  HistoryEnv:
    history_length: 4

  env_action_labels: [NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE]
  num_episodes: 10000000
  num_steps: 10000000
  max_steps_per_episode: 10000
  headless: false
  num_envs: 8

agent:
  name: pong
  gamma: 0.99
  apply_noisy_network: true

  epsilon: 0.9
  epsilon_min: 0.02
  epsilon_linear_decay: 0.000025

  batch_size: 256
  min_memory_size: 128
  loss: huber
  clip_gradients: 100.0

  # Optimizer settings
  optimizer: adam
  learning_rate: 0.0001
  lr_scheduler:
    type: lr_scheduler.LinearLR
    args:
      start_factor: 1
      end_factor: 0.2
      total_iters: 100000

  action_selection: max
  double_dqn: true

  replay_buffer:
    type: prioritized
    size: 65536
    alpha: 0.6
    beta: 0.4
    beta_annealing_steps: 1000000

  replay_every_n_steps: 1
  replays_until_target_update: 200
  checkpoint_every_n_replays: 500

  network:
    dueling_dqn:
      convolution:
        type: 2d
        kernel_sizes: [8, 4, 3]
        channels: [32, 64, 64]
        strides: [4, 2, 1]

      value_hidden_layers: [32,32]
      advantage_hidden_layers: [32,32]

  disabled_curiosity:
    hidden_layers_dim: [200, 200]
    curiosity_reward_weight: 0.4
    curiosity_reward_exponent: 0.4
