device: cuda
use_cuda_amp: true

seed: 42
render_upscale_factor: 2
render_layout: [1, 2]

env:
  env_name: SuperMarioBrosRandomStages-v0
  env_wrappers:
    - RepeatActionEnv
    - CaptureRenderFrameEnv
    - PreprocessFrameEnv
    - HistoryEnv
  ObservationWrapper:
    input: frame
  RepeatActionEnv:
    num_repeat_action: 4
  ClipRewardEnv:
    clip_max: 10.0
    clip_min: -100.0
  CaptureRenderFrameEnv:
    mode: replace
    observation_is_frame: true
  PreprocessFrameEnv:
    resize_shape: [80, 80]
    grayscale: true
    normalize: true
  HistoryEnv:
    history_length: 2

  env_action_labels: ['NOOP', 'right', 'right-jump', 'right-run', 'right-jump-run', 'jump', 'left']
  num_steps: 100_000_000
  max_steps_per_episode: 10_000
  headless: false
  num_envs: 8
  eval_every_n_trained_experiences: 25_000_000
  eval_episodes: 50

agent:
  name: test_agent
  type: value
  gamma: 0.99
  #apply_noisy_network: true

  n_steps: 5

  epsilon: 0.99
  epsilon_min: 0.15
  epsilon_linear_decay: 0.00000002

  batch_size: 256
  min_memory_size: 256
  loss: huber
  clip_gradients: 10.0

  # Optimizer settings
  optimizer: adamw
  learning_rate: 0.00001
  lr_scheduler:
    type: lr_scheduler.LinearLR
    args:
      start_factor: 1
      end_factor: 0.2
      total_iters: 100000

  action_selection: max
  double_dqn: true

  replay_buffer:
    type: uniform
    size: 400_000

  replay_every_n_steps: 1
  replays_until_target_update: 50
  checkpoint_every_n_replays: 10_000

  network:
    transformer_ddqn:
      convolution:
        patches_dim: [10,10]
        token_dimension: 256

      num_heads: 8
      num_layers: 4
      token_dimension: 256

