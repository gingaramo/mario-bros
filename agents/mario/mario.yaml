device: cuda

seed: 42
render_upscale_factor: 2
render_layout: [1, 2]

env:
  env_name: SuperMarioBros-v0
  env_wrappers:
    - RepeatActionEnv
    - CaptureRenderFrameEnv
    - PreprocessFrameEnv
    - ReturnActionEnv
    - HistoryEnv
  ObservationWrapper:
    input: frame
  RepeatActionEnv:
    num_repeat_action: 8
  ClipRewardEnv:
    clip_max: 10.0
    clip_min: -100.0
  CaptureRenderFrameEnv:
    mode: replace
    observation_is_frame: true
  PreprocessFrameEnv:
    resize_shape: [84, 84]
    grayscale: true
    normalize: true
  HistoryEnv:
    history_length: 8

  env_action_labels: ['NOOP', 'right', 'right-jump', 'right-run', 'right-jump-run', 'jump', 'left']
  num_steps: 100_000_000
  max_steps_per_episode: 10_000
  headless: false
  num_envs: 8
  eval_every_n_trained_experiences: 1_000_000
  eval_episodes: 10

agent:
  name: mario
  type: value
  gamma: 0.99
  apply_noisy_network: true

  epsilon: 0.9
  epsilon_min: 0.1
  epsilon_linear_decay: 0.00000125

  batch_size: 512
  min_memory_size: 512
  loss: huber
  clip_gradients: 10.0

  # Optimizer settings
  optimizer: adam
  learning_rate: 0.0001
  lr_scheduler:
    type: lr_scheduler.LinearLR
    args:
      start_factor: 1
      end_factor: 0.2
      total_iters: 100000

  action_selection: max
  double_dqn: true

  replay_buffer:
    type: uniform
    size: 200_000
    beta_annealing_steps: 1000000

  replay_every_n_steps: 1
  replays_until_target_update: 1_000
  checkpoint_every_n_replays: 500_000

  network:
    dueling_dqn:
      convolution:
        type: 2d
        kernel_sizes: [8, 4, 3]
        channels: [32, 64, 64]
        strides: [4, 2, 1]

      value_hidden_layers: [64,64,64,64]
      advantage_hidden_layers: [64,64,64,64]
